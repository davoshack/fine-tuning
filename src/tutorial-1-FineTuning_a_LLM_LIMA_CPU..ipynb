{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: SFT with LoRA\n",
    "\n",
    "â€¢ Find the  [Notebook](https://colab.research.google.com/github/towardsai/ragbook-notebooks/blob/main/notebooks/Chapter%2010%20-%20FineTuning_a_LLM_LIMA_CPU.ipynb)  for this section at  [towardsai.net/book](http://towardsai.net/book).\n",
    "\n",
    "The upcoming section will demonstrate an example of fine-tuning a model using the LoRA approach on a small dataset. This technique allows for instruction tuning of large language models by applying low-rank adaptations to modify the modelâ€™s behavior without retraining its weights. This process is highly efficient and can be executed using a CPU on a Google Cloud instance. We will guide you through preparing the dataset and conducting the fine-tuning process using the Hugging Face library. This hands-on example will provide a practical understanding of enhancing model performance with minimal computational resources and dataset size.\n",
    "\n",
    "While the Hugging Face library is used in this example, the following libraries provide a range of tools, optimizations, compatibility with various data types, resource efficiency, and user-friendly interfaces to support different tasks and hardware setups, enhancing the efficiency of the LLM fine-tuning process.\n",
    "\n",
    "-   [PEFT Library](https://github.com/huggingface/peft): The Parameter-Efficient Fine-Tuning (PEFT) library enables the efficient adaptation of pre-trained language models to various downstream applications without fine-tuning all the modelâ€™s parameters. Methods like LoRA, Prefix Tuning, and P-tuning are part of PEFT.\n",
    "-   [Lit-GPT](https://github.com/Lightning-AI/lit-gpt): Developed by LightningAI, Lit-GPT is also an open-source tool designed to streamline fine-tuning. It facilitates the application of techniques like LoRA without manual modifications to the core model architecture. Models such as  [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/),  [Pythia](https://www.eleuther.ai/papers-blog/pythia-a-suite-for-analyzing-large-language-modelsacross-training-and-scaling), and  [Falcon](https://falconllm.tii.ae/)  are available. Lit-GPT allows applying specific configurations to different weight matrices and offers adjustable precision settings to manage memory usage effectively.\n",
    "\n",
    "We also recommend using virtual machines on systems like GCP, which is good practice in the industry and might be better than using your current system in most cases. To do so, log in to the Google Cloud Platform account and create a  [Compute Engine](https://cloud.google.com/compute)  instance. You can select from various  [machine types](https://cloud.google.com/compute/docs/cpu-platforms). Cloud GPUs are a popular option for many deep learning applications, but CPUs can also effectively optimize LLMs. For this example, we train the model using a CPU.\n",
    "\n",
    "Whatever type of machine you choose to use, if you encounter an out-of-memory error, try reducing parameters such as `batch_size or seq_length`.\n",
    "\n",
    ">âš ï¸**Note:**  Costs are associated with starting up virtual machines. The total cost will depend on the type of machine and how long it is running. Regularly check your costs in the billing section of GCP and turn off your virtual machines when youâ€™re not using them.\n",
    "\n",
    ">ðŸ’¡If you want to run the code in the section without spending much money, you can perform a few iterations of training on your virtual machine and then stop it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the dependences:\n",
    "\n",
    "    !pip install -q transformers==4.32.0 deeplake[enterprise]==3.7.1 trl==0.6.0 peft==0.5.0 wandb==0.15.8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fine_tuning_custom_utils.helper import get_openai_api_key, get_activeloop_api_key\n",
    "OPENAI_API_KEY = get_openai_api_key()\n",
    "ACTIVELOOP_API_KEY = get_activeloop_api_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n",
    "\n",
    "The quality of a modelâ€™s output is directly tied to the quality of the data used for training. Start with a well-planned dataset, whether open-source or custom-created. We will use the dataset from the â€œ[LIMA: Less Is More for Alignment](https://arxiv.org/pdf/2305.11206.pdf)â€ paper. This research suggests that a small, meticulously selected dataset of a thousand samples could replace the RLHF strategy. This research is publicly available under a non-commercial use license.\n",
    "\n",
    "The Deep Lake database infrastructure by Activeloop enables dataset streaming, eliminating the need to download and load the dataset into memory.\n",
    "\n",
    "The code will create a loader object for the training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplake\n",
    "\n",
    "# Connect to the training and testing datasets\n",
    "ds = deeplake.load('hub://genai360/GAIR-lima-train-set')\n",
    "ds_test = deeplake.load('hub://genai360/GAIR-lima-test-set')\n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Dataset(path='hub://genai360/GAIR-lima-train-set', read_only=True, tensors=['answer', 'question', 'source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-trained tokenizer object for the [Open Pre-trained Transformer (OPT)](https://arxiv.org/abs/2205.01068) LLM is initially loaded using the transformers library, and the model is loaded later. We chose OPT for its open availability and relatively moderate parameter count. However, the code in this section is versatile and can be applied to other models. For example, you could use `meta-llama/Llama-2-7b-chat-hf` for [Llama 2](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `prepare_sample_text processes` a row of data stored in Deep Lake, organizing it to start with a question and follow with an answer, separated by two newlines. Defining a formatting function helps the model learn the template and recognize that a prompt beginning with the `question` keyword should typically be completed with an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sample_text(example):\n",
    "    \"\"\"Prepare the text from a sample of the dataset.\"\"\"\n",
    "    text = f\"\"\"Question: {example['question'].text()}\\n\\nAnswer: {example['answer'].text()}\"\"\"\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hugging Face  [TRL library](https://github.com/huggingface/trl)  integrates the SFT method and allows the integration of LoRA configurations, simplifying the implementation.\n",
    "\n",
    "To prepare the training and evaluation datasets for fine-tuning, we use the `ConstantLengthDataset` object from the TLR library. This object requires a tokenizer, the Deep Lake dataset, and a formatting function, `prepare_sample_text`, which processes the data into the required format.\n",
    "\n",
    "The `seq_length` parameter determines the maximum sequence length used during training and evaluation, aligning it with the modelâ€™s configuration. While the value can be set as high as 2048 for models that can handle long contexts, weâ€™ve selected 1024 in this example to optimize memory usage. You may increase this value depending on your datasetâ€™s text length and available resources.\n",
    "\n",
    "Additionally, setting `infinite=True` ensures that once all data points are exhausted, the dataset iterator automatically restarts, making it ideal for continual training scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl.trainer import ConstantLengthDataset\n",
    "\n",
    "train_dataset = ConstantLengthDataset(\n",
    "    tokenizer,\n",
    "    ds,\n",
    "    formatting_func=prepare_sample_text,\n",
    "    infinite=True,\n",
    "    seq_length=1024\n",
    ")\n",
    "\n",
    "eval_dataset = ConstantLengthDataset(\n",
    "    tokenizer,\n",
    "    ds_test,\n",
    "    formatting_func=prepare_sample_text,\n",
    "    seq_length=1024\n",
    ")\n",
    "\n",
    "# Show one sample from train set\n",
    "iterator = iter(train_dataset)\n",
    "sample = next(iterator)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    {'input_ids': tensor([ 2, 45641, 35, ..., 48443, 2517, 742]), 'labels': tensor([ 2, 45641, 35, ..., 48443, 2517, 742])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows that the `ConstantLengthDataset` class took care of all the necessary steps to prepare our dataset.\n",
    "\n",
    ">ðŸ’¡If you use the iterator to print a sample from the dataset, execute the following code to reset the iterator pointer: train_dataset.start_iteration = 0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mastering-fine-tuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
