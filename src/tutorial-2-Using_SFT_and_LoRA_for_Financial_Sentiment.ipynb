{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Using SFT and LoRA for Financial Sentiment\n",
    "\n",
    "‚Ä¢ Find the  [Notebook](https://colab.research.google.com/github/towardsai/ragbook-notebooks/blob/main/notebooks/Chapter%2010%20-%20FineTuning_a_LLM_Financial_Sentiment_CPU.ipynb)  for this section at  [towardsai.net/book](http://towardsai.net/book).\n",
    "\n",
    "We aim to fine-tune an LLM for conducting  **sentiment analysis on financial statements**. The LLM would categorize financial tweets as positive, negative, or neutral. The FinGPT project manages the dataset used in this tutorial. The dataset is a crucial element in this process.\n",
    "\n",
    "A detailed script for implementation and experimentation is included at the end of this chapter.\n",
    "\n",
    "In this tutorial, we‚Äôll fine-tune an LLM on a CPU. This is doable with some specific CPUs with optimizations specifically for common operations used in AI, such as Intel‚Äôs Xeon CPUs with the use of  [Intel Advanced Matrix Extensions](https://www.intel.com/content/www/us/en/products/docs/accelerator-engines/advanced-matrix-extensions/overview.html)  (AMX). For this tutorial, we create a Compute Engine virtual machine with 64 GB of RAM (as we‚Äôll fine-tune the model on CPU) and the previously mentioned CPUs. Both fine-tuning and inference can be accomplished by leveraging its optimization technologies.\n",
    "\n",
    "‚ö†Ô∏èIt‚Äôs important to be aware of the costs associated with virtual machines. The total cost will depend on the machine type and the instance‚Äôs uptime. Regularly check your costs in the billing section of GCP and spin off your instances when you don‚Äôt use them.\n",
    "\n",
    "üí°If you want to run the code in the section without spending much money, you can perform a few iterations of training on your virtual machine and then stop it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n",
    "\n",
    "The FinGPT sentiment dataset includes a collection of financial tweets and their associated labels. The dataset also features an instruction column, typically containing a prompt such as ‚ÄúWhat is the sentiment of the following content? Choose from Positive, Negative, or Neutral.‚Äù\n",
    "\n",
    "We use a smaller subset of the dataset from the Deep Lake database for practicality and efficiency, which already hosts the dataset on its hub.\n",
    "\n",
    "Use the `deeplake.load()` function to create the Dataset object and load the samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplake\n",
    "\n",
    "# Connect to the training and testing datasets\n",
    "ds = deeplake.load('hub://genai360/FingGPT-sentiment-train-set')\n",
    "ds_valid = deeplake.load('hub://genai360/FingGPT-sentiment-valid-set')\n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Dataset(path='hub://genai360/FingGPT-sentiment-train-set', read_only=True, tensors=['input', 'instruction', 'output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, develop a function to format a dataset sample into an appropriate input for the model. Unlike previous methods, this approach includes the instructions at the beginning of the prompt.\n",
    "\n",
    "The format is: `<instruction>\\n\\nContent: <tweet>\\n\\nSentiment: <sentiment>`. The placeholders within <> will be replaced with relevant values from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sample_text(example):\n",
    "    \"\"\"Prepare the text from a sample of the dataset.\"\"\"\n",
    "    text = f\"\"\"{example['instruction'].text()}\\n\\nContent: {example['input'].text()}\\n\\nSentiment: {example['output'].text()}\"\"\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a formatted input derived from an entry in the dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}  \n",
    "      \n",
    "    Content: Diageo Shares Surge on Report of Possible Takeover by Lemann  \n",
    "      \n",
    "    Sentiment: positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the [OPT-1.3B language model](https://huggingface.co/facebook/opt-1.3b) tokenizer and use the ConstantLengthDataset class to create the training and validation dataset. It aggregates multiple samples until a set sequence length threshold is met, improving the efficiency of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n",
    "\n",
    "# Create the ConstantLengthDataset\n",
    "from trl.trainer import ConstantLengthDataset\n",
    "\n",
    "train_dataset = ConstantLengthDataset(\n",
    "    tokenizer,\n",
    "    ds,\n",
    "    formatting_func=prepare_sample_text,\n",
    "    infinite=True,\n",
    "    seq_length=1024\n",
    ")\n",
    "\n",
    "eval_dataset = ConstantLengthDataset(\n",
    "    tokenizer,\n",
    "    ds_valid,\n",
    "    formatting_func=prepare_sample_text,\n",
    "    seq_length=1024\n",
    ")\n",
    "\n",
    "# Show one sample from train set\n",
    "iterator = iter(train_dataset)\n",
    "sample = next(iterator)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    {'input_ids': tensor([50118, 35212, 8913, ..., 2430, 2, 2]),  \n",
    "    'labels': tensor([50118, 35212, 8913, ..., 2430, 2, 2])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí°Before launching the training process, execute the following code to reset the iterator pointer if the iterator is used to print a sample from the dataset: `train_dataset.start_iteration = 0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the Model and Trainer\n",
    "\n",
    "Create a `LoraConfig object`. The `TrainingArguments` class from the transformers library manages the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LoRAConfig\n",
    "from peft import LoraConfig\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Define TrainingArguments\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./OPT-fine_tuned-FinGPT-CPU\",\n",
    "    dataloader_drop_last=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=10,\n",
    "    logging_steps=5,\n",
    "    per_device_train_batch_size=12,\n",
    "    per_device_eval_batch_size=12,\n",
    "    learning_rate=1e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=100,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=False,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    weight_decay=0.05,\n",
    "    ddp_find_unused_parameters=False,\n",
    "    run_name=\"OPT-fine_tuned-FinGPT-CPU\",\n",
    "    report_to=\"wandb\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the OPT-1.3B model in the `bfloat16` format to save on memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"facebook/opt-1.3b\", torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we cast particular layers inside the network to complete 32-bit precision. This improves the model‚Äôs stability during training.\n",
    "\n",
    "üí°‚ÄúCasting‚Äù layers in a model typically refers to changing the data type of the elements within the layers. For example, here, we change them to float 32 for improved precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "for param in model.parameters():\n",
    "  param.requires_grad = False  # freeze the model - train adapters later\n",
    "  if param.ndim == 1:\n",
    "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
    "    param.data = param.data.to(torch.float32)\n",
    "\n",
    "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "  def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect the model, dataset, training arguments, and LoRA configuration using the `SFTTrainer` class. To launch the training process, call the `.train()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=lora_config,\n",
    "    packing=True,\n",
    ")\n",
    "\n",
    "print(\"Training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">üí°Access the best [OPT fine-tuned FinGPT with CPU](https://github.com/towardsai/rag-ebook-files/blob/main/OPT-fine_tuned-FinGPT-CPU.zip) checkpoint at [towardsai.net/book](http://towardsai.net/book). Additionally, find more information on [the Weights & Biases project page](https://wandb.ai/ala_/GenAI360/runs/p08s2n5f?workspace=user-ala_) on the fine-tuning process at [towardsai.net/book](http://towardsai.net/book)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging LoRA and OPT\n",
    "\n",
    "Load and merge the LoRA adaptors from the previous stage with the base model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the base model (OPT-1.3B)\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  \"facebook/opt-1.3b\", return_dict=True, torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load the LoRA adaptors\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load the Lora model\n",
    "model = PeftModel.from_pretrained(model,\n",
    "\"./OPT-fine_tuned-FinGPT-CPU/<desired_checkpoint>/\")\n",
    "model.eval()\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Save for future use\n",
    "model.save_pretrained(\"./OPT-fine_tuned-FinGPT-CPU/merged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "We randomly picked four previously unseen cases from the dataset and fed them into the vanilla base model (OPT-1.3B) and the fine-tuned model using the transformers library‚Äôs `.generate()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"\"\"What is the sentiment of this news? Please choose an answer from {strong negative/moderately negative/mildly negative/neutral/mildly positive/moderately positive/strong positive}, then provide some short reasons.\\n\\n\n",
    "Content: UPDATE 1-AstraZeneca sells rare cancer drug to Sanofi for up to S300 mln.\\n\\nSentiment: \"\"\", return_tensors=\"pt\").to(\"cuda:0\")\n",
    "\n",
    "generation_output = model.generate(**inputs,\n",
    "                                    return_dict_in_generate=True,\n",
    "                                    output_scores=True,\n",
    "                                    max_length=256,\n",
    "                                    num_beams=1,\n",
    "                                    do_sample=True,\n",
    "                                    repetition_penalty=1.5,\n",
    "                                    length_penalty=2.)\n",
    "\n",
    "print(tokenizer.decode(generation_output['sequences'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    What is the sentiment of this news? Please choose an answer from {strong negative/moderately negative/mildly negative/neutral/mildly positive/moderately positive/strong positive}, then provide some short reasons. Content: UPDATE 1-AstraZeneca sells rare cancer drug to Sanofi for up to S300 mln. Sentiment: positive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The samples show that the model fine-tuned on financial tweets performs well. It follows instructions and understands the task at hand. The base model focuses mainly on the default language modeling goal of predicting the next word based on the input.\n",
    "\n",
    "You can find a collection of prompts and their outputs below.\n",
    "\n",
    "1. UPDATE 1-AstraZeneca sells rare cancer drug to Sanofi for up to S300 mln. [Correct Answer: Positive]\n",
    "\n",
    "Vanilla Model Output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    '</s>What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}\\n\\nn\\nContent: UPDATE 1-AstraZeneca sells rare cancer drug to Sanofi for up to $300 mln.\\n\\nSentiment:???????????????(0 votes - average) (100% rating, 12 comments)Rate Article | Share on FacebookTweetThis one looks good and they have a product that can be used in combination with chemo so maybe it will help their patients get rid if these cancers as well as helping them live longer. This doesn't sound like some crap made by Astra at first glance. What do ya'll think about what was announced today though? Do you agree or disagree with these statements??? Also tell us your thoughts below :-)Thanks!Ravi Posted 06.04 2014 1433 words 3+ Comments Upvote 10 downvotes Content Rating = 100% Vote [Article] More content... View vote results page : Popularity Ratio 0.0075962 Shares User Login Signup Google Plus Email / Password Not logged into LinkedIn Join now Comment! A survey just came out compar...'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuned Model Output:\n",
    "\n",
    "    '</s>What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}\\n\\nn\\nContent: UPDATE 1-AstraZeneca sells rare cancer drug to Sanofi for up to $300 mln.\\n\\nSentiment: positive</s>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. SABMiller revenue hit by weaker EM currencies [Correct Answer: Negative]\n",
    "\n",
    "Vanilla Model Output:\n",
    "\n",
    "    '</s>What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}\\n\\nn\\nContent: SABMiller revenue hit by weaker EM currencies\\n\\nSentiment:???????? or Neutral??\\N|B#\\n\\n[Date] : 12 May 2004 @ 20.00 ET (0850 UTC)\\nIn response to a letter published in The Australian today, I can confirm that we have made changes at Millicent for three reasons; Firstly Australia has become one large market since October 2003‚Äôs sales update and second it contains all non-core brands now making up most marketing spend...The third significant action was removing our international unit ‚Äì Australasia & New Zealand Marketing Services [AU&NZSM]. This department had been under pressure before with reduced account allocations but then we realised there were some core issues within AU & NZ so began focusing on these. In turn reducing media budgets significantly whilst seeking increased efficiencies elsewhere...Millisternationals continues tto invest heavily into South American markets ... '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuned Model Output:\n",
    "\n",
    "    '</s>What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}\\n\\nn\\nContent: SABMiller revenue hit by weaker EM currencies\\n\\nSentiment: negative</s>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Buffett‚Äôs Company Reports 37 Percent Drop in 2Q Earnings [Correct Answer: Negative]\n",
    "\n",
    "Vanilla Model Output:\n",
    "\n",
    "    '</s>What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}\\n\\n\\nContent: Buffett\\'s Company Reports 37 Percent Drop in 2Q Earnings\\n\\nSentiment: -5% negative, 20 positives and 12 neutral. Negative responses were most frequent on stocks with either very high revenue estimates or sales that are expected to grow faster than earnings for many years.\" - 1\"The stock market will continue its upward momentum throughout 2008\", was also found by 3 out 4 traders (see tables below). \"Many companies reporting solid results have had excellent year after quarter.\", according 15 trader(note not included) replies... These answers all suggest a belief about future trading conditions based only upon information available since last weeks release which may be distorted due to the markets down session following these trades.(1), (1, 8\")I did NOT see anything indicating higher short interest, but I could imagine some investors looking at what happened when they started long...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuned Model Output:\n",
    "\n",
    "    '</s>What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}\\n\\n\\nContent: Buffett's Company Reports 37 Percent Drop in 2Q Earnings\\n\\nSentiment: negative</s>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. For a few hours this week, the FT gained access to Poly, the university where students in Hong Kong have been trapped‚Ä¶ [Correct Answer: Neutral]\n",
    "\n",
    "Vanilla Model Output:\n",
    "\n",
    "    '</s>What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}\\n\\n\\nContent: For a few hours this week, the FT gained access to PolyU, the university where students in Hong Kong have been trapped... https://t.co/IpIzUTCujW\\n\\nSentiment: (Negative) - many others are saying positive things or that they didn‚Äôt know it existed... ‚Äúthe world's richest woman ‚Äî Li Ka-shing -- donated $2 million and the top job at the University‚Äù! A small group has even gotten together on Facebook to discuss how best we can help her with donations... but there isn't much more done than making those posts; most members want nothing political about donation giving since she would already be doing so anyway given connections among HKers who support pro independence politicians like Chow Kwing Chung II etc.. If everyone wants something politically charged as far you could tell by people commenting for / against anything mentioned then I guess everybody will just assume otherwise if so...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuned Model Output:\n",
    "\n",
    "    '</s>What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}\\n\\n\\nContent: For a few hours this week, the FT gained access to PolyU, the university where students in Hong Kong have been trapped... https://t.co/IpIzUTCujW\\n\\nSentiment: positive</s>'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mastering-fine-tuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
