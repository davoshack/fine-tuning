{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 4: Improving LLMs with RLHF\n",
    "\n",
    "Reinforcement Learning from Human Feedback (RLHF) incorporates human feedback into the training process through a reward model that learns the desired patterns to improve the model‚Äôs output. For example, if the goal is to enhance politeness, the reward model will guide the model to generate more polite responses by assigning higher scores to polite outputs. This process is resource-intensive because it necessitates training a reward model using a dataset curated by humans.\n",
    "\n",
    "This tutorial will use available open-source models and datasets whenever possible while maintaining costs.\n",
    "\n",
    "We begin with a pre-trained model that we fine-tune in a supervised fine-tuning phase using the `SFTTrainer` class. Next, a reward model is trained with the desired traits using the `RewardTrainer` class. Finally, the reinforcement learning phase employs the models to build the ultimate aligned model, utilizing the `PPOTrainer`.\n",
    "\n",
    "You can access the reports generated from Weights & Biases and the file with the requirements for the library after each subsection. Note that different steps require distinct versions of libraries. We chose `OPT-1.3B` as the base model and fine-tuned a `DeBERTa` (300M) model as the reward model for our experiments. While these are more compact models, the process used in this tutorial can be applied to other existing models by simply modifying the model‚Äôs name in the code.\n",
    "\n",
    "Even if much more affordable than what companies like OpenAI do, this tutorial is still resource-intensive as we replicate an RLHF phase. We rented an 8x NVIDIA A100 instance for $8.80/h and used  [lambda](https://lambdalabs.com/)  as our GPU cloud provider.\n",
    "\n",
    ">‚ö†Ô∏èIt‚Äôs important to be aware of the costs associated with cloud GPUs. The total cost will depend on the machine type and the instance‚Äôs uptime. Regularly check your costs in the billing section of Lambda Labs and spin off your instances when you don‚Äôt use them.\n",
    "\n",
    ">üí°If you want to run the code in the section without spending much money, you can perform a few iterations of training on your virtual machine and then stop it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Fine-Tuning\n",
    "\n",
    "‚Ä¢ Find the  [Notebook](https://colab.research.google.com/github/towardsai/ragbook-notebooks/blob/main/notebooks/Chapter%2010%20-%20FineTuning_a_LLM_QLoRA.ipynb)  for this section at  [towardsai.net/book](http://towardsai.net/book).\n",
    "\n",
    "Previous sections covered the SFT phase. This section uses a unique  [OpenOrca](https://huggingface.co/datasets/Open-Orca/OpenOrca)  dataset with question-response pairs and implements the QLoRA fine-tuning technique.\n",
    "\n",
    "This phase teaches the model a conversational format, training it to provide answers rather than defaulting to its standard auto-completion function.\n",
    "\n",
    "Install the required libraries with the command `!pip install -q transformers==4.32.0 bitsandbytes==0.41.1 accelerate==0.22.0 deeplake==3.6.19 trl==0.5.0 peft==0.5.0 wandb==0.15.8.`\n",
    "\n",
    "### The Dataset\n",
    "\n",
    "The first step is streaming the dataset. Streaming a dataset refers to the process of loading and processing data in smaller chunks or batches rather than loading the entire dataset into memory at once. This approach is useful when working with large datasets that cannot fit into memory or when you want to perform real-time processing. For this example, we only use a subset of the original dataset, comprising 1 million data points. However, you can access the  [entire dataset](https://app.activeloop.ai/genai360/OpenOrca-4M/), containing 4 million data points, at  [towardsai.net/book](http://towardsai.net/book)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplake\n",
    "\n",
    "# Connect to the training and testing datasets\n",
    "ds = deeplake.load('hub://genai360/OpenOrca-1M-train-set')\n",
    "ds_valid = deeplake.load('hub://genai360/OpenOrca-1M-valid-set')\n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Dataset(path='hub://genai360/OpenOrca-1M-train-set',  \n",
    "    read_only=True,  \n",
    "    tensors=['id', 'question', 'response', 'system_prompt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset features three key columns: question, the queries posed to the LLM; response, the model‚Äôs output or answers to these questions; and `system_prompt`, the initial instructions that set the context for the model, such as ‚Äúyou are a helpful assistant.‚Äù\n",
    "\n",
    "For simplicity, this chapter focuses solely on the first two columns. However, incorporating system prompts into text formatting can be advantageous. The text is structured in the format `Question: xxx\\n\\nAnswer: yyy`, with the question and answer separated by two newline characters. You can also experiment with different formats, such as `System: xxx\\n\\nQuestion: yyy\\n\\nAnswer: zzz`, to include the system prompts from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sample_text(example):\n",
    "    \"\"\"Prepare the text from a sample of the dataset.\"\"\"\n",
    "    text = f\"\"\"Question: {example['question'][0]}\\n\\nAnswer: {example['response'][0]}\"\"\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, load the OPT model tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `ConstantLengthDataset` class to aggregate data. This will maximize usage within the 2K input size constraint and improve training efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl.trainer import ConstantLengthDataset\n",
    "\n",
    "train_dataset = ConstantLengthDataset(\n",
    "    tokenizer,\n",
    "    ds,\n",
    "    formatting_func=prepare_sample_text,\n",
    "    infinite=True,\n",
    "    seq_length=2048\n",
    ")\n",
    "\n",
    "eval_dataset = ConstantLengthDataset(\n",
    "    tokenizer,\n",
    "    ds_valid,\n",
    "    formatting_func=prepare_sample_text,\n",
    "    seq_length=1024\n",
    ")\n",
    "\n",
    "iterator = iter(train_dataset)\n",
    "sample = next(iterator)\n",
    "print(sample)\n",
    "\n",
    "train_dataset.start_iteration = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    {'input_ids': tensor([ 16, 358, 828, ..., 137, 79, 362]),  \n",
    "    'labels': tensor([ 16, 358, 828, ..., 137, 79, 362])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Model and Trainer\n",
    "\n",
    "The following code sets the LoRA configuration. The parameter r=16 sets the rank of the LoRA layers, controlling the reduction in dimensionality. The `lora_alpha=32` parameter adjusts the scaling factor for the adaptation, impacting how much the new parameters contribute to the model. `lora_dropout=0.05` specifies a dropout rate of 5%, adding regularization during training. The `bias=\"none\"` parameter indicates no bias is added to the model. Finally, `task_type=\"CAUSAL_LM\"` defines the task type as causal language modeling, which is typically used for autoregressive text generation models. We‚Äôll later load the model in a quantized mode, effectively implementing QLoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the `TrainingArguments`, which define the hyperparameters of the training process:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./OPT-fine_tuned-OpenOrca\",\n",
    "    dataloader_drop_last=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    num_train_epochs=2,\n",
    "    eval_steps=2000,\n",
    "    save_steps=2000,\n",
    "    logging_steps=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=1e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=100,\n",
    "    gradient_accumulation_steps=1,\n",
    "    bf16=True,\n",
    "    weight_decay=0.05,\n",
    "    ddp_find_unused_parameters=False,\n",
    "    run_name=\"OPT-fine_tuned-OpenOrca\",\n",
    "    report_to=\"wandb\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a `BitsAndBytes` configuration. This new class package runs the quantization operation and loads the model in a 4-bit format. We will use the NF4 data type for weights and the nested quantization strategy to reduce memory usage while maintaining performance.\n",
    "\n",
    "Next, specify that the training process computations be carried out in the `bfloat16` format.\n",
    "\n",
    "The QLoRA method integrates LoRA with quantization to optimize memory usage further. To enable this functionality, include the `quantization_config` when initializing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `AutoModelForCasualLM` class to load the OPT model‚Äôs pre-trained weights containing 1.3 billion parameters. Note that the model will be loaded using the GPUs registered in the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from accelerate import Accelerator\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  \"facebook/opt-1.3b\",\n",
    "    quantization_config=quantization_config,\n",
    "    device_map={\"\": Accelerator().process_index}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the model architecture before initializing the trainer object to improve its efficiency. This requires casting specific layers of the model to complete precision (32 bits), including LayerNorms and the final language modeling head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "for param in model.parameters():\n",
    "  param.requires_grad = False\n",
    "  if param.ndim == 1:\n",
    "    param.data = param.data.to(torch.float32)\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "  def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SFTTrainer` class will begin training using the initialized dataset, the model, and the training arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=lora_config,\n",
    "    packing=True,\n",
    ")\n",
    "\n",
    "print(\"Training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SFTTrainer` instance will automatically establish checkpoints during the training process, as given by the `save_steps` argument (from `TrainingArguments`), and save them to the `./OPT-fine_tuned-OpenOrca` directory.\n",
    "\n",
    "Merge the LoRA layers with the base model to form a standalone network. The following code will handle the merging process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  \"facebook/opt-1.3b\", return_dict=True, torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load the Lora model\n",
    "model = PeftModel.from_pretrained(model, \"./OPT-fine_tuned-OpenOrca/<step>\")\n",
    "model.eval()\n",
    "\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "model.save_pretrained(\"./OPT-fine_tuned-OpenOrca/merged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standalone model will be accessible on the .`/OPT-supervised_fine_tuned/merged` directory. This checkpoint will be used in the Reinforcement Learning section.\n",
    "\n",
    ">üí°[The Merged Model Checkpoint (2GB)](https://drive.google.com/file/d/1D9rH2kLiBgRR31xvelOcW09uHzrO5Fbv/view?usp=drive_link), [Weights & Bias Report](https://wandb.ai/ala_/GenAI360/runs/n6czwaqq?workspace=user-ala_), and the [fine-tuning requirements](https://github.com/towardsai/rag-ebook-files/blob/main/requirements-fine-tune.txt) are accessible at [towardsai.net/book](http://towardsai.net/book).\n",
    "\n",
    ">(The provided requirements text file is a snapshot of all the packages on the server; not all of these packages are necessary for you)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Reward Model\n",
    "\n",
    "‚Ä¢ Find the  [Notebook](https://colab.research.google.com/github/towardsai/ragbook-notebooks/blob/main/notebooks/Chapter%2010%20-%20FineTuning_Reward_Model.ipynb)  for this section at  [towardsai.net/book](http://towardsai.net/book).\n",
    "\n",
    "The reward model is designed to learn human preferences from labeled examples, guiding the LLM during the final stage of the RLHF process. It is exposed to examples of preferred and less desirable behaviors. It learns to mirror human preferences by assigning higher scores to preferred examples.\n",
    "\n",
    "In essence, reward models perform a classification task, choosing the better option from a pair of sample interactions based on human feedback. Various network architectures can be used as reward models. A key consideration is whether the reward model should be similar to the base model to ensure it has adequate knowledge for practical guidance. However, smaller models such as DeBERTa or RoBERTa have also demonstrated efficiency. If resources permit, exploring larger models can be beneficial.\n",
    "\n",
    "Install the essential libraries with the command `!pip install -q transformers==4.32.0 deeplake==3.6.19 sentencepiece==0.1.99 trl==0.6.0.`\n",
    "\n",
    "### The Dataset\n",
    "\n",
    ">üí°Note that the datasets in this step contain inappropriate language and offensive words. This approach aligns the model‚Äôs behavior by instructing the model not to replicate it.\n",
    "\n",
    "For the RLHF process, we use the ‚Äú[helpfulness/harmless](https://github.com/anthropics/hh-rlhf)‚Äù dataset from Anthropic. This dataset is tailored for RLHF and offers an in-depth understanding of the approach. Find  [the study](https://arxiv.org/abs/2204.05862)  and the dataset at  [towardsai.net/book](http://towardsai.net/book).\n",
    "\n",
    "The following code will set up the data loader objects for the training and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplake\n",
    "\n",
    "ds = deeplake.load('hub://genai360/Anthropic-hh-rlhf-train-set')\n",
    "ds_valid = deeplake.load('hub://genai360/Anthropic-hh-rlhf-test-set')\n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset(path='hub://genai360/Anthropic-hh-rlhf-train-set', read_only=True, tensors=['chosen', 'rejected'])\n",
    "\n",
    "Before structuring the dataset for the Trainer class, load the pre-trained tokenizer for DeBERTa (the reward model). The code should be recognizable; the `AutoTokenizer` class will locate the suitable initializer class and utilize the `.from_pretrained()` method to load the pre-trained tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch‚Äôs `Dataset` class prepares the dataset for various downstream tasks. A pair of inputs is required to train a reward model. The first item will represent the selected (favorable) conversation, while the second will represent a talk rejected by labelers. The reward model will allocate a higher score to the chosen sample and a lower score to the rejected samples.\n",
    "\n",
    "The code below tokenizes the samples and combines them into a single Python dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "      chosen = self.dataset.chosen[idx].text()\n",
    "      rejected = self.dataset.rejected[idx].text()\n",
    "\n",
    "      tokenized_chosen = tokenizer(chosen, truncation=True, max_length=512, padding='max_length')\n",
    "      tokenized_rejected = tokenizer(rejected, truncation=True, max_length=512, padding='max_length')\n",
    "\n",
    "      formatted_input = {\n",
    "        \"input_ids_chosen\": tokenized_chosen[\"input_ids\"],\n",
    "        \"attention_mask_chosen\": tokenized_chosen[\"attention_mask\"],\n",
    "        \"input_ids_rejected\": tokenized_rejected[\"input_ids\"],\n",
    "        \"attention_mask_rejected\": tokenized_rejected[\"attention_mask\"],\n",
    "      }\n",
    "\n",
    "      return formatted_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Trainer` class requires a dictionary with four keys for training. This includes the tokenized forms for chosen and rejected talks (`input_ids_chosen` and `input_ids_rejected`) and their respective attention masks `(attention_mask_chosen and attention_mask_rejected)`. Attention masks are necessary because they add padding tokens to standardize input sizes (up to the model‚Äôs maximum input size of 512 in this example), which warns the model that specific tokens at the end do not contain valuable information and can be ignored.\n",
    "\n",
    "You can use the previously established class to construct an instance of the dataset or extract a single row from the dataset using the iter and next methods to validate the output keys and ensure that everything works as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(ds)\n",
    "eval_dataset = MyDataset(ds_valid)\n",
    "\n",
    "# Print one sample row\n",
    "iterator = iter(train_dataset)\n",
    "one_sample = next(iterator)\n",
    "print(list(one_sample.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ['input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected',  \n",
    "    'attention_mask_rejected']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Model and Trainer\n",
    "\n",
    "Import the pre-trained DeBERTa model using the `AutoModelForSequenceClassification`. Set the number of labels (`num_labels`) to 1 since just a single score is needed to evaluate the quality of a sequence. A high score will signify content alignment, while a low score suggests the content may be unsuitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"microsoft/deberta-v3-base\", num_labels=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of `TrainingArguments`, setting the intended hyperparameters. You can explore various hyperparameters based on the selection of pre-trained models and available resources. For example, if an out-of-memory error is encountered, a smaller batch size might be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"DeBERTa-reward-hh_rlhf\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.001,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    gradient_accumulation_steps=1,\n",
    "    bf16=True,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    optim=\"adamw_hf\",\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    ddp_find_unused_parameters=False,\n",
    "    run_name=\"DeBERTa-reward-hh_rlhf\",\n",
    "    report_to=\"wandb\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `RewardTrainer` class from the TRL library integrates all components, including the previously defined elements, such as the model, tokenizer, and dataset, and executes the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import RewardTrainer\n",
    "\n",
    "trainer = RewardTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `trainer` will automatically save the checkpoints, which will be used in the. Reinforcement Learning section.\n",
    "\n",
    "üí°[The Reward Model Checkpoint (Step 1000 - 2GB)](https://drive.google.com/file/d/1GWL-ayeeXDCMuqYjvDzgPJiRFmBY6X6Z/view?usp=drive_link), [Weights & Biases report](https://wandb.ai/ala_/GenAI360/runs/tqamj3nw?workspace=user-ala_), and [Requirements](https://github.com/towardsai/rag-ebook-files/blob/main/requirements-reward.txt) are accessible at [towardsai.net/book](http://towardsai.net/book).\n",
    "\n",
    "(The provided requirements text file is a snapshot of all the packages on the server; not all of these packages are necessary for you)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "\n",
    "‚Ä¢ Find the  [Notebook](https://colab.research.google.com/github/towardsai/ragbook-notebooks/blob/main/notebooks/Chapter%2010%20-%20FineTune_RLHF.ipynb)  for this section at  [towardsai.net/book](http://towardsai.net/book).\n",
    "\n",
    "This final step in RLHF involves integrating the models we have developed earlier. At this point, the focus is on using the reward model trained earlier to align the fine-tuned model more closely with human feedback. During the training loop, a custom prompt will elicit a response from the fine-tuned OPT model. The reward model will then evaluate this response, assigning a score based on its resemblance to a response a human might generate.\n",
    "\n",
    "In this reinforcement learning phase, safeguards ensure the model maintains the knowledge it has acquired and remains true to the original model‚Äôs foundational principles. The next step involves introducing the dataset, followed by an in-depth exploration of the process in the following subsections.\n",
    "\n",
    "Install the necessary libraries with the command `!pip install -q transformers==4.32.0 accelerate==0.22.0 peft==0.5.0 trl==0.5.0 bitsandbytes==0.41.1 deeplake==3.6.19 wandb==0.15.8 sentencepiece==0.1.99`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset\n",
    "\n",
    "There is considerable flexibility in selecting the dataset for this phase. The distinctive feature of this approach is that the reward model evaluates outputs independently of any specific labels, so the learning process doesn‚Äôt require a question-answer format.\n",
    "\n",
    "We will use the OpenOrca dataset provided by Alpaca, a subset of the larger  [OpenOrca](https://huggingface.co/datasets/Open-Orca/OpenOrca)  dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplake\n",
    "\n",
    "# Connect to the training and testing datasets\n",
    "ds = deeplake.load('hub://genai360/Alpaca-OrcaChat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Dataset(path='hub://genai360/Alpaca-OrcaChat', read_only=True,  \n",
    "    tensors=['input', 'instruction', 'output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of three columns: `input`, the user‚Äôs prompt to the model; `instruction`, the directive for the model; and output, the model‚Äôs response. For the RL process, we will focus solely on the input column.\n",
    "\n",
    "Before establishing a dataset class for appropriate formatting, load the pre-trained tokenizer corresponding to the fine-tuned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\", padding_side='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trainer will need the query in its original and tokenized text formats in the following section. Therefore, the query will be retained as text, while the input_ids will signify the token IDs. Note that the query variable is a template for creating user prompts. This is structured in the format `Question: XXX\\n\\nAnswer:,` consistent with the one used during the SFT phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, ds):\n",
    "        self.ds = ds\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "      query = \"Question: \" + self.ds.input[idx].text() + \"\\n\\nAnswer: \"\n",
    "      tokenized_question = tokenizer(query, truncation=True,\n",
    "max_length=400, padding='max_length', return_tensors=\"pt\")\n",
    "\n",
    "      formatted_input = {\n",
    "        \"query\": query,\n",
    "        \"input_ids\": tokenized_question[\"input_ids\"][0],\n",
    "      }\n",
    "\n",
    "      return formatted_input\n",
    "\n",
    "# Define the dataset object\n",
    "myTrainingLoader = MyDataset(ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a collator function to convert individual samples from the data loader into data batches. This function will be provided to the `Trainer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the SFT Models\n",
    "\n",
    "Import the fine-tuned model, designated as `OPT-supervised_fine_tuned`, using the settings from the `PPOConfig` class. Most of these parameters have been previously discussed in earlier parts of the book. However, `adapt_kl_ctrl` and `init_kl_coef` require attention. They manage the Kullback‚ÄìLeibler (KL) divergence penalty, a crucial factor in ensuring the model does not diverge excessively from the pre-trained version and prevents it from producing nonsensical sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import PPOConfig\n",
    "\n",
    "config = PPOConfig(\n",
    "    task_name=\"OPT-RL-OrcaChat\",\n",
    "    steps=10_000,\n",
    "    model_name=\"./OPT-fine_tuned-OpenOrca/merged\",\n",
    "    learning_rate=1.41e-5,\n",
    "    batch_size=32,\n",
    "    mini_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optimize_cuda_cache=True,\n",
    "    early_stopping=False,\n",
    "    target_kl=0.1,\n",
    "    ppo_epochs=4,\n",
    "    seed=0,\n",
    "    init_kl_coef=0.2,\n",
    "    adap_kl_ctrl=True,\n",
    "    tracker_project_name=\"GenAI360\",\n",
    "    log_with=\"wandb\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `set_seed()` function to set the random state for repeatability. The `current_device` variable will save your device ID, which will be used later in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import set_seed\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# set seed before initializing value head for deterministic eval\n",
    "set_seed(config.seed)\n",
    "\n",
    "# Now let's build the model, the reference model, and the tokenizer.\n",
    "current_device = Accelerator().local_process_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code loads the SFT model by configuring the LoRA process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the LoRA configuration with the `AutoModelForCausalLMwithValueHead` class to load the pre-trained weights. We use the `load_in_8bit` parameter to load the model, which uses a quantization technique that reduces weight precision. This helps to preserve memory during model training. This model is intended for use in the reinforcement learning loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import AutoModelForCausalLMWithValueHead\n",
    "\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    config.model_name,\n",
    "    load_in_8bit=True,\n",
    "    device_map={\"\": current_device},\n",
    "    peft_config=lora_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Reward Model\n",
    "\n",
    "The Hugging Face pipeline simplifies the process of loading the Reward model.\n",
    "\n",
    "First, specify the task at hand. For our tutorial, we chose `sentiment-analysis`, which aligns with our primary binary classification goal. Next, select the path to the pre-trained reward model using the model parameter. If a pre-trained reward model is available on the Hugging Face Hub, use the model‚Äôs name from there.\n",
    "\n",
    "The pipeline will automatically load the proper tokenizer, and we can start categorization by feeding any text into the designated object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "reward_pipeline = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"./DeBERTa-v3-base-reward-hh_rlhf/checkpoint-1000\",\n",
    "    tokenizer=\"./DeBERTa-v3-base-reward-hh_rlhf/checkpoint-1000\",\n",
    "    device_map={\"\": current_device},\n",
    "    model_kwargs={\"load_in_8bit\": True},\n",
    "    return_token_type_ids=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `reward_pipe` variable, which contains the reward model, will be used during the reinforcement learning training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proximal Policy Optimization Training\n",
    "\n",
    "Use the Proximal Policy Optimization (PPO) to improve the stability of the training loop. PPO limits changes to the model, avoiding overly large updates. Observations show that making more minor, gradual adjustments can speed up the convergence of the training process.\n",
    "\n",
    "Before starting the actual training loop, it‚Äôs necessary to define certain variables for their integration within this loop.\n",
    "\n",
    "First, set up the `output_length_sampler` object. This object is responsible for generating samples within a specific range. In this case, from a minimum to a maximum number of tokens. Our objective is to have outputs ranging between 32 to 400 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl.core import LengthSampler\n",
    "\n",
    "output_length_sampler = LengthSampler(32, 400) #(OutputMinLength, OutputMaxLength)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establish two dictionaries to manage the generation process for the fine-tuned and reward models. These dictionaries configure various parameters governing each network‚Äôs sampling process, truncation, and batch size during the inference stage. Specify the `save_freq` variable, which dictates the frequency at which checkpoints are saved during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_gen_kwargs = {\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "    \"eos_token_id\": 100_000,\n",
    "}\n",
    "\n",
    "reward_gen_kwargs = {\n",
    "    \"top_k\": None,\n",
    "    \"function_to_apply\": \"none\",\n",
    "    \"batch_size\": 16,\n",
    "    \"truncation\": True,\n",
    "    \"max_length\": 400\n",
    "}\n",
    "\n",
    "save_freq = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the PPO trainer object using the `PPOTrainer` class. This requires the PPOConfig instance, the directory of the fine-tuned model, and the training dataset as inputs.\n",
    "\n",
    "There is also an option to supply a reference model via the `ref_model` parameter. This model acts as a benchmark for the KL divergence penalty. If this parameter is not specified, the trainer will automatically default to using the original pre-trained model as the reference point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import PPOTrainer\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config,\n",
    "    model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=myTrainingLoader,\n",
    "    data_collator=collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop‚Äôs final component starts by acquiring a single batch of samples to generate responses from the fine-tuned model using the `input_ids`. These responses are decoded, combined with the initial prompt, and provided to the reward model. The reward model evaluates these responses, assigning scores based on how closely they resemble human responses.\n",
    "\n",
    "Finally, the PPO object will update the model weights based on the reward model‚Äôs scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "for step, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "    if step >= config.total_ppo_epochs:\n",
    "        break\n",
    "    question_tensors = batch[\"input_ids\"]\n",
    "\n",
    "    response_tensors = ppo_trainer.generate(\n",
    "        question_tensors,\n",
    "        return_prompt=False,\n",
    "        length_sampler=output_length_sampler,\n",
    "        **sft_gen_kwargs,\n",
    "    )\n",
    "    batch[\"response\"] = tokenizer.batch_decode(response_tensors,\n",
    "skip_special_tokens=True)\n",
    "\n",
    "    # Compute reward score\n",
    "    texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "    pipe_outputs = reward_pipeline(texts, **reward_gen_kwargs)\n",
    "\n",
    "    rewards = [torch.tensor(output[0][\"score\"]) for output in pipe_outputs]\n",
    "\n",
    "    # Run PPO step\n",
    "    stats = ppo_trainer.step(question_tensors, response_tensors, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)\n",
    "\n",
    "    if save_freq and step and step % save_freq == 0:\n",
    "        print(\"Saving checkpoint.\")\n",
    "        ppo_trainer.save_pretrained(f\"./OPT-RL-OrcaChat/checkpoint-{step}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the LoRA adaptors with the base model to use the network independently. Edit the directory of the saved checkpoint adapter based on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  \"facebook/opt-1.3b\", return_dict=True, torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load the Lora model\n",
    "model = PeftModel.from_pretrained(model, \"./OPT-RL-OrcaChat/checkpoint-400/\")\n",
    "model.eval();\n",
    "\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "model.save_pretrained(\"./OPT-RL-OrcaChat/merged\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí°[The Merged RL Model Checkpoint (2GB)](https://drive.google.com/file/d/12ekdlETljGrZm_SP50us3XRXIKEMn0C-/view?usp=drive_link), [Weights & Biases report](https://wandb.ai/ala_/GenAI360/runs/e9y58bdi?workspace=user-ala_), and [Requirements](https://github.com/towardsai/rag-ebook-files/blob/main/requirements-rl.txt) are accessible at [towardsai.net/book](http://towardsai.net/book).\n",
    "\n",
    ">(The provided requirements text file is a snapshot of all the packages on the server; not all of these packages are necessary for you)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "The fine-tuned model‚Äôs outputs can be evaluated using a range of prompts. The following code uses Hugging Face‚Äôs `.generate()` method for easy interaction with models.\n",
    "\n",
    "Load the tokenizer and the model and decode the produced output. The beam search decoding method is used for this process, with a restriction set to produce no more than 128 tokens. You can learn more about these techniques further in the  [blog post](https://huggingface.co/blog/how-to-generate)  by Hugging Face (available at  [towardsai.net/book](http://towardsai.net/book))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "from accelerate import Accelerator\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./OPT-RL-OrcaChat/merged\", device_map={\"\": Accelerator().process_index}\n",
    ")\n",
    "model.eval();\n",
    "\n",
    "inputs = tokenizer(\"\"\"Question: In one sentence, describe what the following article is about:\\n\\nClick on ‚ÄúStore‚Äù along the menu toolbar at the upper left of the screen. Click on ‚ÄúSign In‚Äù from the drop-down menu and enter your Apple ID and password. After logging in, click on ‚ÄúStore‚Äù on the toolbar again and select ‚ÄúView Account‚Äù from the drop-down menu. This will open the Account Information page.  Click on the drop-down list and select the country you want to change your iTunes Store to.  You‚Äôll now be directed to the iTunes Store welcome page. Review the Terms and Conditions Agreement and click on ‚ÄúAgree‚Äù if you wish to proceed. Click on ‚ÄúContinue‚Äù once you‚Äôre done to complete changing your iTunes Store..\\n\\n Answer: \"\"\",\n",
    "return_tensors=\"pt\").to(\"cuda:0\")\n",
    "generation_output = model.generate(**inputs,\n",
    "                                   return_dict_in_generate=True,\n",
    "                                   output_scores=True,\n",
    "                                   max_new_tokens=128,\n",
    "                                   num_beams=4,\n",
    "                                   do_sample=True,\n",
    "                                   top_k=10,\n",
    "                                   temperature=0.6)\n",
    "print(tokenizer.decode(generation_output['sequences'][0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following entries represent the outputs generated by the model using various prompts:\n",
    "1. In one sentence, describe what the following article is about:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(generation_output['sequences'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    '<s>Question: In one sentence, describe what the following article is about:\\n\\nClick on \"Store\" along the menu toolbar at the upper left of the screen. Click on \"Sign In\" from the drop-down menu and enter your Apple ID and password. After logging in, click on \"Store\" on the toolbar again and select \"View Account\" from the drop-down menu. This will open the Account Information page. Click on the drop-down list and select the country you want to change your iTunes Store to. You'll now be directed to the iTunes Store welcome page. Review the Terms and Conditions Agreement and click on \"Agree\" if you wish to proceed. Click on \"Continue\" once you're done to complete changing your iTunes Store.\\n\\nAnswer: The article is about how to change your iTunes Store country.</s>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Answer the following question given in this paragraph:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(generation_output['sequences'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    '<s>Question: Answer the following question given in this paragraph: When a wave meets a barrier, it reflects and travels back the way it came. The reflected wave may interfere with the original wave. If this occurs in precisely the right way, a standing wave can be created. The types of standing waves that can form depend strongly on the speed of the wave and the size of the region in which it is traveling. Q: A standing wave is created when what type of wave interferes with the original wave? A: ina). realized wave b). translated wave c). refracted wave d). reflected wave\\n\\nAnswer: A</s>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What the following paragraph is about:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(generation_output['sequences'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    '<s>Question: What the following paragraph is about? Rain is water droplets that have condensed from atmospheric water vapor and then fall under gravity. Rain is a major component of the water cycle and is responsible for depositing most of the fresh water on the Earth. It provides water for hydroelectric power plants, crop irrigation, and suitable conditions for many types of ecosystems.\\n\\nAnswer: √Ä Rain is water droplets that have condensed</s>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. What the following paragraph is about (different example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(generation_output['sequences'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    '<s>Question: What the following paragraph is about? friendship, a state of enduring affection, esteem, intimacy, and trust between two people. In all cultures, friendships are important relationships throughout a person's life span. In some cultures, the concept of friendship is restricted to a small number of very deep relationships; in others, such as the U.S. and Canada, a person could have many friends, and perhaps a more intense relationship with one or two people, who may be called good friends or best friends. Other colloquial terms include besties or Best Friends Forever (BFFs). Although there are many forms of friendship, certain features are common to many such bonds, such as choosing to be with one another, enjoying time spent together, and being able to engage in a positive and supportive role to one another.\\n\\nAnswer: ________\\n\\nQuestion: What the following paragraph is about? friendship, a state of enduring affection, esteem, intimacy,</s>'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The examples show the model‚Äôs proficiency in following instructions and extracting information from extensive content. Yet, it has some limitations in responding to open-ended. This is mainly due to the model‚Äôs smaller scale; larger models are about 30 to 70 times larger."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mastering-fine-tuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
